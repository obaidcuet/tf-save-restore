{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn import svm\n",
    "import tensorflow as tf\n",
    "\n",
    "import os \n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "tf.set_random_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw training set\n",
    "labeled_images = pd.read_csv('../input/train.csv')\n",
    "\n",
    "# data preparation\n",
    "from sklearn.preprocessing import LabelBinarizer # for one hot encoding\n",
    "encoder = LabelBinarizer()\n",
    "width = height = 28 # image resulation 28X28\n",
    "images = labeled_images.iloc[:,1:]/255 # normalize values between 0 to 1 and take only image data (exclude label)\n",
    "images = np.reshape(np.array(images), (-1, width, height, 1)) # reshape to 28X28 like pixel, -1 for unlimited rows, 1 for monochrome\n",
    "labels = encoder.fit_transform(labeled_images.iloc[:,:1]) # take label and one-hot-encode\n",
    "\n",
    "# split given traing to further train and test sets\n",
    "train_images, test_images,train_labels, test_labels = train_test_split(images, labels, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders\n",
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"X\")\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10], name=\"Y_\")\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32, name=\"lr\")\n",
    "# dropout probability\n",
    "pkeep = tf.placeholder(tf.float32, name=\"pkeep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer's related variables\n",
    "# three convolutional layers with their channel counts, and a\n",
    "# fully connected layer (the last layer has 10 softmax neurons)\n",
    "# try another value(24, 48, 64, 200)\n",
    "K = 6  # first convolutional layer output depth 24\n",
    "L = 24  # second convolutional layer output depth 48\n",
    "M = 48  # third convolutional layer 64\n",
    "N = 600  # fully connected layer 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "# make sure weights and biasses are NOT initialized with zeros\n",
    "# convolution layers\n",
    "with tf.name_scope('conv_layer1'): # tensorboard: using namespace\n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1), name=\"W1\")  # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]), name=\"B1\")\n",
    "    stride = 1  # output is 28x28\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "\n",
    "\n",
    "with tf.name_scope('conv_layer2'): # tensorboard: using namespace\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1), name=\"W2\")\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]), name=\"B2\")\n",
    "    stride = 2  # output is 14x14\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "\n",
    "\n",
    "with tf.name_scope('conv_layer3'): # tensorboard: using namespace\n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1), name=\"W3\")\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]), name=\"B3\")\n",
    "    stride = 2  # output is 7x7\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "# fully connected\n",
    "with tf.name_scope('fc_layer2'): # tensorboard: using namespace\n",
    "    W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1), name=\"W4\")\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]), name=\"B4\")\n",
    "    Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "\n",
    "# outout\n",
    "with tf.name_scope('output_layer'): # tensorboard: using namespace\n",
    "    W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1), name=\"W5\")\n",
    "    B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]), name=\"B5\")\n",
    "    Ylogits = tf.matmul(YY4, W5) + B5\n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss/error mesurement\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_) # Note: need to use Ylogits here, instead og Y\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# training step\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "    \n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training related variables\n",
    "learning_rate = 0.001\n",
    "percent_keep = 0.25\n",
    "train_data_length = len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess=tf.Session() \n",
    "sess.run(init)\n",
    "\n",
    "# restore\n",
    "saver = tf.train.Saver()\n",
    "save_path ='E:\\myroot\\work\\data_science\\kaggle\\digit-recognizer\\codes\\saved_models\\digit-recognizer_v2.0'\n",
    "saver.restore(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final accuracy of test dataset\n",
    "# predict labels\n",
    "test_pred_labels = np.argmax(sess.run(Y, feed_dict={X: test_images, pkeep: 1.0}),axis=1)\n",
    "\n",
    "# predict accuracy\n",
    "sum(test_pred_labels==np.argmax(test_labels, axis=1))/len(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

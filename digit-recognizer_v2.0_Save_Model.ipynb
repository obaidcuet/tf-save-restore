{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn import svm\n",
    "import tensorflow as tf\n",
    "\n",
    "import os \n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard: event log location\n",
    "LOGDIR = r'E:\\myroot\\work\\data_science\\kaggle\\digit-recognizer\\tensorboard_log\\28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw training set\n",
    "labeled_images = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "from sklearn.preprocessing import LabelBinarizer # for one hot encoding\n",
    "encoder = LabelBinarizer()\n",
    "width = height = 28 # image resulation 28X28\n",
    "images = labeled_images.iloc[:,1:]/255 # normalize values between 0 to 1 and take only image data (exclude label)\n",
    "images = np.reshape(np.array(images), (-1, width, height, 1)) # reshape to 28X28 like pixel, -1 for unlimited rows, 1 for monochrome\n",
    "labels = encoder.fit_transform(labeled_images.iloc[:,:1]) # take label and one-hot-encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split given traing to further train and test sets\n",
    "train_images, test_images,train_labels, test_labels = train_test_split(images, labels, train_size=0.8, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders\n",
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"X\")\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10], name=\"Y_\")\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32, name=\"lr\")\n",
    "# dropout probability\n",
    "pkeep = tf.placeholder(tf.float32, name=\"pkeep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer's related variables\n",
    "# three convolutional layers with their channel counts, and a\n",
    "# fully connected layer (the last layer has 10 softmax neurons)\n",
    "# try another value(24, 48, 64, 200)\n",
    "K = 6  # first convolutional layer output depth 24\n",
    "L = 24  # second convolutional layer output depth 48\n",
    "M = 48  # third convolutional layer 64\n",
    "N = 600  # fully connected layer 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "# make sure weights and biasses are NOT initialized with zeros\n",
    "# convolution layers\n",
    "with tf.name_scope('conv_layer1'): # tensorboard: using namespace\n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1), name=\"W1\")  # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]), name=\"B1\")\n",
    "    stride = 1  # output is 28x28\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    # tensorboard: adding histogram\n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", B1)\n",
    "\n",
    "with tf.name_scope('conv_layer2'): # tensorboard: using namespace\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1), name=\"W2\")\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]), name=\"B2\")\n",
    "    stride = 2  # output is 14x14\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    # tensorboard: adding histogram\n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", B2)\n",
    "\n",
    "with tf.name_scope('conv_layer3'): # tensorboard: using namespace\n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1), name=\"W3\")\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]), name=\"B3\")\n",
    "    stride = 2  # output is 7x7\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "    # tensorboard: adding histogram\n",
    "    tf.summary.histogram(\"weight3\", W3)\n",
    "    tf.summary.histogram(\"bias3\", B3)\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "# fully connected\n",
    "with tf.name_scope('fc_layer2'): # tensorboard: using namespace\n",
    "    W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1), name=\"W4\")\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]), name=\"B4\")\n",
    "    Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    # tensorboard: adding histogram\n",
    "    tf.summary.histogram(\"weight4\", W4)\n",
    "    tf.summary.histogram(\"bias4\", B4)\n",
    "\n",
    "# outout\n",
    "with tf.name_scope('output_layer'): # tensorboard: using namespace\n",
    "    W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1), name=\"W5\")\n",
    "    B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]), name=\"B5\")\n",
    "    Ylogits = tf.matmul(YY4, W5) + B5\n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "    # tensorboard: adding histogram\n",
    "    tf.summary.histogram(\"weight5\", W5)\n",
    "    tf.summary.histogram(\"bias5\", B5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss/error mesurement\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_) # Note: need to use Ylogits here, instead og Y\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "    tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training step\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard: merge all summary\n",
    "summ = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create meaningfull string with related param name to point a subdirectory under LOGDIR and also for printing/logging\n",
    "def make_hparam_string(learning_rate, hum_fc_layer, num_conv_layer):\n",
    "    conv_param = \"conv=\"+ str(num_conv_layer)\n",
    "    fc_param = \"fc=\"+str(hum_fc_layer)\n",
    "    return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training related variables\n",
    "learning_rate = 0.001\n",
    "percent_keep = 0.25\n",
    "train_data_length = len(train_images)\n",
    "\n",
    "# params string for current run\n",
    "hparam = make_hparam_string(learning_rate=learning_rate, hum_fc_layer=2, num_conv_layer=3)\n",
    "\n",
    "# ops for initialize global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# create tf session\n",
    "sess = tf.Session()\n",
    "\n",
    "# tensorboard: initiate writer\n",
    "# train: all matrix here including train accuracy & loss\n",
    "writer_train = tf.summary.FileWriter(os.path.join(LOGDIR , hparam, 'train'))\n",
    "writer_train.add_graph(sess.graph)\n",
    "\n",
    "# test: test accuracy & loss\n",
    "writer_test = tf.summary.FileWriter(os.path.join(LOGDIR , hparam, 'test'))\n",
    "\n",
    "# initialize global variables\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "steps = 50 # how many times to apply entire training dataset\n",
    "batch_size = 600 # train_data_length  should be dividable by this\n",
    "\n",
    "for step in range(steps):  \n",
    "    # foward pass\n",
    "    for i in range(0, train_data_length, batch_size):\n",
    "        # get next batch data from the randomize index\n",
    "        batch_X = train_images[i:i+batch_size]\n",
    "        batch_Y = train_labels[i:i+batch_size]\n",
    "     \n",
    "        # train with the batch\n",
    "        sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, pkeep: percent_keep})\n",
    "\n",
    "    # backward pass\n",
    "    for i in range(train_data_length-1, -1, batch_size):\n",
    "        # get next batch data from the randomize index\n",
    "        batch_X = train_images[i:i+batch_size]\n",
    "        batch_Y = train_labels[i:i+batch_size]\n",
    "     \n",
    "        # train with the batch\n",
    "        sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, pkeep: percent_keep})\n",
    "    \n",
    "    # collect stats after one epoch (1 fwd + 1 bkw pass)\n",
    "    train_accuracy, train_loss, s_train = sess.run([accuracy, cross_entropy, summ], {X: train_images, Y_: train_labels, pkeep: 1.0})\n",
    "    writer_train.add_summary(s_train, i)\n",
    "    \n",
    "    test_accuracy, test_loss, s_test = sess.run([accuracy, cross_entropy, summ], {X: test_images, Y_: test_labels, pkeep: 1.0})\n",
    "    writer_test.add_summary(s_test, i)\n",
    "    # print stats\n",
    "    print (\"Train Step {}:: Accuracy: train {} test {} :: Loss: train {} test {}\".format(step, train_accuracy, test_accuracy, train_loss, test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final accuracy of test dataset\n",
    "# predict labels\n",
    "test_pred_labels = np.argmax(sess.run(Y, feed_dict={X: test_images, pkeep: 1.0}),axis=1)\n",
    "\n",
    "# predict accuracy\n",
    "sum(test_pred_labels==np.argmax(test_labels, axis=1))/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "save_path ='E:\\myroot\\work\\data_science\\kaggle\\digit-recognizer\\codes\\saved_models\\digit-recognizer_v2.0'\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitiate all variables with default values\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final accuracy of test dataset with default values\n",
    "# predict labels\n",
    "test_pred_labels = np.argmax(sess.run(Y, feed_dict={X: test_images, pkeep: 1.0}),axis=1)\n",
    "\n",
    "# predict accuracy\n",
    "sum(test_pred_labels==np.argmax(test_labels, axis=1))/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore in the same jupyter session\n",
    "saver.restore(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final accuracy of test dataset with restored model\n",
    "# predict labels\n",
    "test_pred_labels = np.argmax(sess.run(Y, feed_dict={X: test_images, pkeep: 1.0}),axis=1)\n",
    "\n",
    "# predict accuracy\n",
    "sum(test_pred_labels==np.argmax(test_labels, axis=1))/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for submit\n",
    "# read test dataset\n",
    "unlabeled_images = pd.read_csv('../input/test.csv')\n",
    "\n",
    "# preprocess\n",
    "from sklearn.preprocessing import LabelBinarizer # for one hot encoding\n",
    "encoder = LabelBinarizer()\n",
    "width = height = 28 # image resulation 28X28\n",
    "submit_images = unlabeled_images/255 # normalize values between 0 to 1 and take only image data (exclude label)\n",
    "submit_images = np.reshape(np.array(submit_images), (-1, width, height, 1)) # reshape to 28X28 like pixel, -1 for unlimited rows, 1 for monochrome\n",
    "\n",
    "print(unlabeled_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels for submit\n",
    "submit_pred_labels = np.argmax(sess.run(Y, feed_dict={X: submit_images, pkeep: 1.0}),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission file\n",
    "submission = pd.DataFrame(data={'ImageId':(np.arange(submit_pred_labels.shape[0])+1), 'Label':submit_pred_labels})\n",
    "submission.to_csv('submission_v1.9.csv', index=False)\n",
    "submission.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
